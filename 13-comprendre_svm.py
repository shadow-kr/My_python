from sklearn import datasets
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from sklearn.utils.testing import assert_equal

iris = datasets.load_iris()

X, y = iris.data, iris.target
ovr = OneVsRestClassifier(LinearSVC(random_state=0, multi_class='ovr')).fit(X, y)
# For the answer of why OneVsRestClassifier wrapper and multi_class='ovr', look below!

# I used LinearSVC() instead of SVC(kernel='linear') for this particular problem.
# They are basically the same just with some implementation differences according to docs.
# https://stackoverflow.com/questions/45384185/what-is-the-difference-between-linearsvc-and-svckernel-linear

ovr.n_classes_
#number of classes
#3

ovr.estimators_
#Number of estimators must equal to num_classes

# [LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
#            intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#            multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
#            verbose=0),
#  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
#            intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#            multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
#            verbose=0),
#  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
#            intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#            multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
#            verbose=0)]

ovr.predict(X) 
# array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,
#        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
#        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,
#        2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

y
# array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,
#        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
#        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,
#        2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

ovr.score(X, y)
#Returns the mean accuracy on the given test data and labels.
#0.9666666666666667

#decision_function is only used with a SVM classifier reason being it gives out the distance of your data points from the hyperplane that separates the data

# How to interpret decision function?
# https://stackoverflow.com/questions/20113206/scikit-learn-svc-decision-function-and-predict/20114601
ovr.decision_function(X)
#The desion function tells us on which side of the hyperplane generated by the classifier we are (and how far we are away from it). 
#Based on that information, the estimator then label the examples with the corresponding label.

import matplotlib.pyplot as plt
plt.scatter(X[:,0], X[:,2])
plt.title('Linearly separable data')
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()